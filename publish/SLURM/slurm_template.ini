
################################
## Cluster Configuration File ##
################################

[cluster SLURM-Cluster]
FormLayout = selectionpanel
Category = Schedulers

Autoscale = $Autoscale

    [[node defaults]]
    UsePublicNetwork = $UsePublicNetwork
    Credentials = $Credentials    
    SubnetId = $SubnetId
    Region = $Region
    KeyPairLocation = ~/.ssh/cyclecloud.pem
    
    # Slurm autoscaling supports both Terminate and Deallocate shutdown policies
    ShutdownPolicy = $configuration_slurm_shutdown_policy

        [[[configuration]]]
        slurm.version = $configuration_slurm_version
        slurm.accounting.enabled = $configuration_slurm_accounting_enabled
        slurm.accounting.url = $configuration_slurm_accounting_url
        slurm.accounting.user = $configuration_slurm_accounting_user
        slurm.accounting.password = $configuration_slurm_accounting_password
        slurm.additional.config = $additional_slurm_config

        # For fast spin-up after Deallocate, force an immediate re-converge on boot
        cyclecloud.converge_on_boot = true

        # Disable normal NFS exports and mounts
        cyclecloud.mounts.sched.disabled = true
        cyclecloud.mounts.shared.disabled = true
        cyclecloud.exports.sched.disabled = true
        cyclecloud.exports.shared.disabled = true
        cyclecloud.exports.sched.samba.enabled = false
        cyclecloud.exports.shared.samba.enabled = false
        cyclecloud.exports.defaults.samba.enabled = false      
        cshared.server.legacy_links_disabled = true

        [[[configuration cyclecloud.hosts.standalone_dns]]]
        enabled = true

        [[[cluster-init cyclecloud/slurm:default]]]
        Optional = true

        [[[configuration cyclecloud.mounts.nfs_shared]]]
        type = nfs
        mountpoint = /shared
        export_path = $NFSSharedExportPath
        address = $NFSAddress
        options = $NFSSharedMountOptions

        [[[configuration cyclecloud.mounts.nfs_sched]]]
        type = nfs
        mountpoint = /sched

        [[[configuration cyclecloud.mounts.additional_nfs]]]
        disabled = ${AdditionalNAS isnt true}
        type = nfs
        address = $AdditonalNFSAddress
        mountpoint = $AdditionalNFSMountPoint
        export_path = $AdditionalNFSExportPath
        options = $AdditionalNFSMountOptions

    [[node scheduler]]
    MachineType = $SchedulerMachineType
    ImageName = $SchedulerImageName
    IsReturnProxy = $ReturnProxy
    AdditionalClusterInitSpecs = $SchedulerClusterInitSpecs
    CloudInit = '''#!/bin/bash
###### NEXTFLOW INSTALLATION ######
installNextflow="Yes"
if [ $installNextflow == "Yes" ]; then
    # Check if Java is installed
    if ! type java > /dev/null; then
        echo "Java is not installed. Installing it..."
        sudo apt install -y default-jre
    fi

    # Install nextflow 
    if ! type nextflow > /dev/null; then
        echo "Nextflow is not installed. Installing it..."
        # Un-comment the following line if you want to install the version of nextflow for Azure Batch
        #export NXF_EDGE=1 
        curl -s https://get.nextflow.io | bash
        # Make nextflow available to all users
        cp ./nextflow /usr/local/bin
        nextflow self-update
        # Enable all users to execute nextflow
        chmod a+rx /usr/local/bin/nextflow
        # Trigger the update of nextflow with an empty first invocation
        nextflow
    else
        echo "Nextflow is already installed. Updating it in case a new version is available..."
        nextflow self-update
    fi

    # Install nf-core
    if ! type nf-core > /dev/null; then
        echo "nf-core is not installed. Installing it..."
        pip install --upgrade pip
        pip install nf-core
    else
        echo "nf-core is already installed. Updating it in case a new version is available..."
        pip install --upgrade pip
        pip install nf-core --upgrade
    fi
fi
#######################################

##### CONDA INSTALLATION ######
installConda="Yes"
if [ $installConda == "Yes" ]; then
    # Check if Conda is installed
    if ! type conda > /dev/null; then
        echo "Conda not installed. Installing it..."
        CONDA_ENVS_PATH="/tmp/conda-envs"
        echo "Installing into: $CONDA_ENVS_PATH/bin"
        #   Download the latest installer for Linux and then run the installation, specifying the path for conda environment
        #   (-f skips error of folder already existing)
        wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
        mkdir -p .conda
        mkdir -p $CONDA_ENVS_PATH
        echo "Starting the installation of Conda..."
        echo "Setting the HOME variable to avoid the installer to fail..."
        export HOME=/tmp
        bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p $CONDA_ENVS_PATH
        #   Initialise Conda
        echo "Initialising conda..."
        conda init
        #   Apply the path to the Conda binaries to the PATH environment variable
        echo "Adding the path to the Conda binaries to the PATH environment variable in /etc/environment..."
        echo PATH=\"$CONDA_ENVS_PATH/bin:${PATH}\" | tee -a /etc/environment
        #   Finally, set the default path for environments
        echo "Setting the default path for environments..."
        conda config --append envs_dirs $CONDA_ENVS_PATH
    else
        echo "Conda is already installed. Updating it in case a new version is available..."
        conda update conda
    fi
fi
#######################################

###### NVMe DISK SETUP    ###########
###### & DOCKER DATA MOVE ###########
# Check if the VM has an NVMe disk
if [ -b /dev/nvme0n1 ]; then
    echo "NVMe disk found. Preparing a partition and mounting it..."
    # Create a partition
    # sudo parted -s /dev/nvme0n1 mklabel gpt
    # sudo parted -s /dev/nvme0n1 mkpart primary 0% 100%
    # Format the partition
    sudo mkfs.ext4 /dev/nvme0n1
    # Run partprobe to update the kernel partition table with the new drive
    sudo partprobe /dev/nvme0n1
    # Create a mount point
    sudo mkdir -p /nvmedrive
    # Mount the partition
    sudo mount /dev/nvme0n1 /nvmedrive
    # Add the mount point to the fstab file so that it is mounted automatically after a reboot
    echo "/dev/nvme0n1 /nvmedrive ext4 defaults,nofail 0 2" | sudo tee -a /etc/fstab
    # Make the mount point accessible to all users
    sudo chmod -R 777 /nvmedrive
else
    echo "NVMe disk not found. Skipping..."
fi

# If the NVMe disk exists, move the Docker data directory to the NVMe disk
if [ -b /dev/nvme0n1 ]; then
    sudo mkdir -p /nvmedrive/docker
    sudo systemctl stop docker
    sudo mv /var/lib/docker /nvmedrive/docker
    sudo ln -s /nvmedrive/docker /var/lib/docker
    sudo systemctl start docker
fi
#######################################

########## DOCKER FIX #################
# Fix access to docker.sock
# Check if docker is installed
if ! type docker > /dev/null; then
    echo "Docker not installed. Skipping..."
else
    echo "Docker is installed. Fixing access to docker.sock..."
    sudo chmod 777 /var/run/docker.sock
fi
#######################################
'''
    
        [[[configuration]]]
        cyclecloud.mounts.nfs_sched.disabled = true
        cyclecloud.mounts.nfs_shared.disabled = ${NFSType != "External"}
        cyclecloud.discoverable = true

        [[[cluster-init cyclecloud/slurm:scheduler]]]

        [[[network-interface eth0]]]
        AssociatePublicIpAddress = $UsePublicNetwork
        StaticPublicIpAddress = true

        [[[input-endpoint ganglia]]]
        PrivatePort = 8652
        PublicPort = 8652

        [[[volume sched]]]
        Size = 30
        Mount = builtinsched
        Persistent = false
        StorageAccountType = Premium_LRS

        [[[volume shared]]]
        Size = $SharedDiskSize
        StorageAccountType = Premium_LRS
        Mount = builtinshared
        Persistent = false

        [[[configuration cyclecloud.mounts.builtinsched]]]
        mountpoint = /sched
        fs_type = xfs

        [[[configuration cyclecloud.mounts.builtinshared]]]
        disabled = ${NFSType != "Builtin"}
        mountpoint = /shared
        fs_type = xfs

        [[[configuration cyclecloud.exports.builtinsched]]]
        export_path = /sched
        options = no_root_squash
        samba.enabled = false
        type = nfs

        [[[configuration cyclecloud.exports.builtinshared]]]
        disabled = ${NFSType != "Builtin"}
        export_path = /shared
        samba.enabled = false
        type = nfs

    [[node nodearraybase]]
    Abstract = true

        [[[configuration]]]
        slurm.autoscale = true
        slurm.use_nodename_as_hostname = false

        [[[cluster-init cyclecloud/slurm:execute]]]

        [[[network-interface eth0]]]
        AssociatePublicIpAddress = $ExecuteNodesPublic

    [[nodearray hpc]]
    Extends = nodearraybase
    MachineType = $HPCMachineType
    ImageName = $HPCImageName
    MaxCoreCount = $MaxHPCExecuteCoreCount
    Azure.MaxScalesetSize = $HPCMaxScalesetSize
    AdditionalClusterInitSpecs = $HPCClusterInitSpecs
    CloudInit = '''#!/bin/bash
##### CONDA INSTALLATION ######
installConda="Yes"
if [ $installConda == "Yes" ]; then
    # Check if Conda is installed
    if ! type conda > /dev/null; then
        echo "Conda not installed. Installing it..."
        CONDA_ENVS_PATH="/tmp/conda-envs"
        echo "Installing into: $CONDA_ENVS_PATH/bin"
        #   Download the latest installer for Linux and then run the installation, specifying the path for conda environment
        #   (-f skips error of folder already existing)
        wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
        mkdir -p .conda
        mkdir -p $CONDA_ENVS_PATH
        echo "Starting the installation of Conda..."
        echo "Setting the HOME variable to avoid the installer to fail..."
        export HOME=/tmp
        bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p $CONDA_ENVS_PATH
        #   Initialise Conda
        echo "Initialising conda..."
        conda init
        #   Apply the path to the Conda binaries to the PATH environment variable
        echo "Adding the path to the Conda binaries to the PATH environment variable in /etc/environment..."
        echo PATH=\"$CONDA_ENVS_PATH/bin:${PATH}\" | tee -a /etc/environment
        #   Finally, set the default path for environments
        echo "Setting the default path for environments..."
        conda config --append envs_dirs $CONDA_ENVS_PATH
    else
        echo "Conda is already installed. Updating it in case a new version is available..."
        conda update conda
    fi
fi
#######################################

###### NVMe DISK SETUP    ###########
###### & DOCKER DATA MOVE ###########
# Check if the VM has an NVMe disk
if [ -b /dev/nvme0n1 ]; then
    echo "NVMe disk found. Preparing a partition and mounting it..."
    # Create a partition
    # sudo parted -s /dev/nvme0n1 mklabel gpt
    # sudo parted -s /dev/nvme0n1 mkpart primary 0% 100%
    # Format the partition
    sudo mkfs.ext4 /dev/nvme0n1
    # Run partprobe to update the kernel partition table with the new drive
    sudo partprobe /dev/nvme0n1
    # Create a mount point
    sudo mkdir -p /nvmedrive
    # Mount the partition
    sudo mount /dev/nvme0n1 /nvmedrive
    # Add the mount point to the fstab file so that it is mounted automatically after a reboot
    echo "/dev/nvme0n1 /nvmedrive ext4 defaults,nofail 0 2" | sudo tee -a /etc/fstab
    # Make the mount point accessible to all users
    sudo chmod -R 777 /nvmedrive
else
    echo "NVMe disk not found. Skipping..."
fi

# If the NVMe disk exists, move the Docker data directory to the NVMe disk
if [ -b /dev/nvme0n1 ]; then
    sudo mkdir -p /nvmedrive/docker
    sudo systemctl stop docker
    sudo mv /var/lib/docker /nvmedrive/docker
    sudo ln -s /nvmedrive/docker /var/lib/docker
    sudo systemctl start docker
fi
#######################################

########## DOCKER FIX #################
# Fix access to docker.sock
# Check if docker is installed
if ! type docker > /dev/null; then
    echo "Docker not installed. Skipping..."
else
    echo "Docker is installed. Fixing access to docker.sock..."
    sudo chmod 777 /var/run/docker.sock
fi
#######################################
'''


        [[[configuration]]]
        slurm.default_partition = true
        slurm.hpc = true
        slurm.partition = hpc

        [[[volume boot]]]
        Size = $HPCWorkerOSDiskSize
        StorageAccountType = Premium_LRS
        Persistent = false

[parameters About]
Order = 1

    [[parameters About Slurm]]

        [[[parameter slurm]]]
        HideLabel = true
        Config.Plugin = pico.widget.HtmlTemplateWidget
        Config.Template := "<table role=\"presentation\"><tr><td><img alt=\"Slurm icon\" src='static/cloud/cluster/ui/ClusterIcon/slurm.png' width='192' height='192'></td></tr><tr><td><p>Slurm is a highly configurable open source workload manager. See the <a href=\"https://www.schedmd.com/\" target=\"_blank\">Slurm project site</a> for an overview.</p><p>Follow the instructions in the <a href=\"https://github.com/azure/cyclecloud-slurm/\" target=\"_blank\">README</a> for details on instructions on extending and configuring the Project for your environment.</p></td></tr></table>"

[parameters Required Settings]
Order = 10

    [[parameters Virtual Machines ]]
    Description = "The cluster, in this case, has two roles: the scheduler node with shared filer and the execute hosts. Configure which VM types to use based on the requirements of your application."
    Order = 20

        [[[parameter Region]]]
        Label = Region
        Description = Deployment Location
        ParameterType = Cloud.Region

        [[[parameter SchedulerMachineType]]]
        Label = Scheduler VM Type
        Description = The VM type for scheduler node
        ParameterType = Cloud.MachineType
        DefaultValue = Standard_B2ms

        [[[parameter HPCMachineType]]]
        Label = HPC VM Type
        Description = The VM type for HPC execute nodes
        ParameterType = Cloud.MachineType
        DefaultValue = Standard_B8ms

    [[parameters Auto-Scaling]]
    Description = "The cluster can autoscale to the workload, adding execute hosts as jobs are queued. To enable this check the box below and choose the initial and maximum core counts for the cluster"
    Order = 30

        [[[parameter Autoscale]]]
        Label = Autoscale
        DefaultValue = true
        Widget.Plugin = pico.form.BooleanCheckBox
        Widget.Label = Start and stop execute instances automatically

        [[[parameter MaxHPCExecuteCoreCount]]]
        Label = Max HPC Cores
        Description = The total number of HPC execute cores to start
        DefaultValue = 100
        Config.Plugin = pico.form.NumberTextBox
        Config.MinValue = 1
        Config.IntegerOnly = true

        [[[parameter HPCMaxScalesetSize]]]
        Label = Max VMs per Scaleset
        Description = The maximum number of VMs created per VM Scaleset e.g. switch in Slurm.
        DefaultValue = 40
        Config.Plugin = pico.form.NumberTextBox
        Config.MinValue = 1
        Config.IntegerOnly = true

    [[parameters Networking]]
    Order = 40

        [[[parameter SubnetId]]]
        Label = Subnet ID
        Description = Subnet Resource Path (ResourceGroup/VirtualNetwork/Subnet)
        ParameterType = Azure.Subnet
        Required = True

[parameters Network Attached Storage]
Order = 15

    [[parameters Default NFS Share]]
    Order = 10
        [[[parameter About shared]]]
        HideLabel = true
        Config.Plugin = pico.widget.HtmlTemplateWidget
        Config.Template := "<p>The directory <code>/shared</code> is a network attached mount and exists in all nodes of the cluster. Users' home directories reside within this mountpoint with the base homedir <code>/shared/home</code>.<br><br>There are two options for providing this mount:<br> <strong>[Builtin]</strong>: The scheduler node is an NFS server that provides the mountpoint to the other nodes of the cluster.<br> <strong>[External NFS]</strong>: A network attached storage such as Azure Netapp Files, HPC Cache, or another VM running an NFS server, provides the mountpoint.</p>"
        Order = 20

        [[[parameter NFSType]]]
        Label = NFS Type
        ParameterType = StringList
        Config.Label = Type of NFS to use for this cluster
        Config.Plugin = pico.form.Dropdown
        Config.Entries := {[Label="External NFS"; Value="External"], [Label="Builtin"; Value="Builtin"]}
        DefaultValue = Builtin

        [[[parameter NFSAddress]]]
        Label = NFS IP Address
        Description = The IP address or hostname of the NFS server. Also accepts a list comma-separated addresses, for example, to mount a frontend load-balanced Azure HPC Cache.
        Config.ParameterType = String
        Conditions.Hidden := NFSType != "External"

        [[[parameter NFSSharedExportPath]]]
        Label = Shared Export Path
        Description = The path exported by the file system
        DefaultValue = /shared
        Conditions.Hidden := NFSType != "External"

        [[[parameter NFSSharedMountOptions]]]
        Label = NFS Mount Options
        Description = NFS Client Mount Options
        Conditions.Hidden := NFSType != "External"

        [[[parameter SharedDiskSize]]]
        Label = Size (GB)
        Description = The size of the NFS share
        DefaultValue = 10

        Config.Plugin = pico.form.NumberTextBox
        Config.MinValue = 10
        Config.MaxValue = 10240
        Config.IntegerOnly = true
        Conditions.Excluded := NFSType != "Builtin"

    [[parameters Additional NFS Mount]]
    Order = 20
        [[[parameter Additional NFS Mount Readme]]]
        HideLabel = true
        Config.Plugin = pico.widget.HtmlTemplateWidget
        Config.Template := "<p>Mount another NFS endpoint on the cluster nodes</p>"
        Order = 20

        [[[parameter AdditionalNAS]]]
        HideLabel = true
        DefaultValue = false
        Widget.Plugin = pico.form.BooleanCheckBox
        Widget.Label = Add NFS mount

        [[[parameter AdditonalNFSAddress]]]
        Label = NFS IP Address 
        Description = The IP address or hostname of the NFS server. Also accepts a list comma-separated addresses, for example, to mount a frontend load-balanced Azure HPC Cache.
        Config.ParameterType = String
        Conditions.Excluded := AdditionalNAS isnt true

        [[[parameter AdditionalNFSMountPoint]]]
        Label = NFS Mount Point
        Description = The path at which to mount the Filesystem
        DefaultValue = /data
        Conditions.Excluded := AdditionalNAS isnt true

        [[[parameter AdditionalNFSExportPath]]]
        Label = NFS Export Path
        Description = The path exported by the file system
        DefaultValue = /data
        Conditions.Excluded := AdditionalNAS isnt true

        [[[parameter AdditionalNFSMountOptions]]]
        Label = NFS Mount Options
        Description = NFS Client Mount Options
        Conditions.Excluded := AdditionalNAS isnt true

[parameters Advanced Settings]
Order = 20

    [[parameters Azure Settings]]
    Order = 10 

        [[[parameter Credentials]]]
        Description = The credentials for the cloud provider
        ParameterType = Cloud.Credentials

    [[parameters Slurm Settings ]]
    Description = "Section for configuring Slurm"
    Order = 5

        [[[parameter configuration_slurm_version]]]
        Required = True
        Label = Slurm Version
        Description = Version of Slurm to install on the cluster
        ParameterType = StringList
        Config.Plugin = pico.form.Dropdown
        Config.FreeForm = true
        Config.Entries := {[Value="20.11.7-1"]}
        DefaultValue = 20.11.7-1

        [[[parameter configuration_slurm_accounting_enabled]]]
        Label = Job Accounting
        DefaultValue = false
        Widget.Plugin = pico.form.BooleanCheckBox
        Widget.Label = Configure Slurm job accounting

        [[[parameter configuration_slurm_accounting_url]]]
        Label = Slurm DBD URL
        Description = URL of the database to use for Slurm job accounting
        Conditions.Excluded := configuration_slurm_accounting_enabled isnt true

        [[[parameter configuration_slurm_accounting_user]]]
        Label = Slurm DBD User
        Description = User for Slurm DBD admin
        Conditions.Excluded := configuration_slurm_accounting_enabled isnt true

        [[[parameter configuration_slurm_accounting_password]]]
        Label = Slurm DBD Password
        Description = Password for Slurm DBD admin
        ParameterType = Password
        Conditions.Excluded := configuration_slurm_accounting_enabled isnt true
        
        [[[parameter configuration_slurm_shutdown_policy]]]
        Label = ShutdownPolicy
        description = By default, autostop will Delete stopped VMS for lowest cost.  Optionally, Stop/Deallocate the VMs for faster restart instead.
        DefaultValue = Terminate
        config.plugin = pico.control.AutoCompleteDropdown
            [[[[list Config.Entries]]]]
            Name = Terminate
            Label = Terminate
            [[[[list Config.Entries]]]]
            Name = Deallocate
            Label = Deallocate

        [[[parameter additional_slurm_config]]]
        Label = Additional Slurm configuration
        Description = Any additional lines to add to slurm.conf
        ParameterType = Text

    [[parameters Software]]
    Description = "Specify the scheduling software, and base OS installed on all nodes, and optionally the cluster-init and chef versions from your Locker."
    Order = 10

        [[[parameter SchedulerImageName]]]
        Label = Scheduler OS
        ParameterType = Cloud.Image
        Config.OS = linux
        DefaultValue = cycle.image.ubuntu18
        Config.Filter := Package in {"cycle.image.centos7", "cycle.image.centos8", "cycle.image.ubuntu18"}

        [[[parameter HPCImageName]]]
        Label = HPC OS
        ParameterType = Cloud.Image
        Config.OS = linux
        DefaultValue = cycle.image.ubuntu18
        Config.Filter := Package in {"cycle.image.centos7", "cycle.image.centos8", "cycle.image.ubuntu18"}

        [[[parameter SchedulerClusterInitSpecs]]]
        Label = Scheduler Cluster-Init
        DefaultValue = =undefined
        Description = Cluster init specs to apply to the scheduler node
        ParameterType = Cloud.ClusterInitSpecs
        
        [[[parameter HPCClusterInitSpecs]]]
        Label = HPC Cluster-Init
        DefaultValue = =undefined
        Description = Cluster init specs to apply to HPC execute nodes
        ParameterType = Cloud.ClusterInitSpecs

    [[parameters Advanced Networking]]
    Description = Advanced networking settings

        [[[parameter ReturnProxy]]]
        Label = Return Proxy
        DefaultValue = true
        ParameterType = Boolean
        Config.Label = Use SSH tunnel to connect to CycleCloud (required if direct access is blocked)

        [[[parameter UsePublicNetwork]]]
        Label = Public Head Node
        DefaultValue = true
        ParameterType = Boolean
        Config.Label = Access scheduler node from the Internet

        [[[parameter ExecuteNodesPublic]]]
        Label = Public Execute
        DefaultValue = false
        ParameterType = Boolean
        Config.Label = Access execute nodes from the Internet
        Conditions.Excluded := UsePublicNetwork isnt true
    
